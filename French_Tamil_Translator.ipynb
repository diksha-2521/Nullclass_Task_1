{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3eade4",
   "metadata": {},
   "source": [
    "# French → Tamil Translator\n",
    "This notebook uses a small slice of the <a href=\"https://opus.nlpl.eu/XLEnt/fr&ta/v1.2/XLEnt\">https://opus.nlpl.eu/XLEnt/fr&ta/v1.2/XLEnt<a> to build a translation model.\n",
    "\n",
    "## Loading the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9176b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, Bidirectional, Dropout, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82f0dd6-d137-428b-b953-6c37fc4ab560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9083487643978985882\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2238133044\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10033659875732934849\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67e55e-4758-4610-9c4b-01ce84ca7127",
   "metadata": {},
   "source": [
    "## Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b8b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of French sentences 14810\n",
      "No. of Tamil sentences 14810\n"
     ]
    }
   ],
   "source": [
    "french_file = \"fr-ta/XLEnt.fr-ta.fr\"\n",
    "tamil_file = \"fr-ta/XLEnt.fr-ta.ta\"\n",
    "\n",
    "\n",
    "with open(french_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    fr_sentences = f.read().splitlines()\n",
    "\n",
    "\n",
    "with open(tamil_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    tam_sentences = f.read().splitlines()\n",
    "\n",
    "# Making a list that contains only 5-letter words\n",
    "french_sentences = [sentence for sentence in fr_sentences if len(sentence.strip()) == 5]\n",
    "tamil_sentences = [tam_sentences[fr_sentences.index(sentence)] for sentence in french_sentences]\n",
    "\n",
    "# Verifying that the lists are equal in length \n",
    "print(\"No. of French sentences\",len(french_sentences))\n",
    "print(\"No. of Tamil sentences\",len(tamil_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63543e01-ee1c-47f5-90eb-3bdd21157f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jésus ', 'Moïse ', 'de US ', 'Chine ', 'Allah ']\n",
      "['இயேசு ', 'மூஸா ', 'அமெரிக்க ', 'சீனா ', 'அல்லாஹ் ']\n"
     ]
    }
   ],
   "source": [
    "# Verifying the the translation match up\n",
    "print(french_sentences[0:5])\n",
    "print(tamil_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169a59c5-c893-4faf-86ee-b43eb6781f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15393 French words\n",
      "13471 unique French words\n",
      "10 Most common words in the French dataset:\n",
      "\"de\" \"la\" \"Hôtel\" \"Certs\" \")\" \"genre\" \"A\" \"du\" \"Allah\" \"ville\"\n",
      "\n",
      "16114 Tamil words\n",
      "12136 unique Tamil words\n",
      "10 Most common words in the Tamil dataset:\n",
      "\"சிலர்\" \"ஜெயலலிதா\" \"சென்னை\" \"பாபா\" \"பாகிஸ்தான்\" \"போலீஸ்\" \"ராஜா\" \")\" \"ஹோட்டலில்\" \"Pearson\"\n"
     ]
    }
   ],
   "source": [
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "tamil_words_counter = collections.Counter([word for sentence in tamil_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} French words'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')\n",
    "\n",
    "print()\n",
    "print('{} Tamil words'.format(len([word for sentence in tamil_sentences for word in sentence.split()])))\n",
    "print('{} unique Tamil words'.format(len(tamil_words_counter)))\n",
    "print('10 Most common words in the Tamil dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*tamil_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf2883-e2f6-4f82-b565-d621cdab8340",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7010156-54b1-4e9a-aa08-5a65c98cac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max French sentence length: 3\n",
      "Max Tamil sentence length: 7\n",
      "French vocabulary size: 12820\n",
      "Tamil vocabulary size: 12011\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x),tokenizer\n",
    "def pad(x,length=None):\n",
    "    if length==None:\n",
    "        length = max(len(sentence) for sentence in x)\n",
    "    return pad_sequences(x,maxlen=length,padding='post')\n",
    "\n",
    "def preprocess(x,y):\n",
    "    preprocess_x,tk_x = tokenize(x)\n",
    "    preprocess_y,tk_y = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    return preprocess_x,preprocess_y,tk_x,tk_y\n",
    "\n",
    "\n",
    "preproc_french_sentences,preproc_tamil_sentences,french_tokenizer,tamil_tokenizer = preprocess(french_sentences,tamil_sentences)\n",
    "\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "max_tamil_sequence_length = preproc_tamil_sentences.shape[1]\n",
    "french_vocab_size = len(french_tokenizer.word_index)+1\n",
    "tamil_vocab_size = len(tamil_tokenizer.word_index)+1\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"Max Tamil sentence length:\", max_tamil_sequence_length)\n",
    "print(\"French vocabulary size:\", french_vocab_size)\n",
    "print(\"Tamil vocabulary size:\", tamil_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e69e4-6e85-4631-a6ca-44dd40e0da8b",
   "metadata": {},
   "source": [
    "## Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5b352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 7, 32)             410240    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 7, 64)            12672     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 7, 256)           16640     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 256)            0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 7, 12011)         3086827   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,526,379\n",
      "Trainable params: 3,526,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "371/371 [==============================] - 27s 57ms/step - loss: 1.8364 - accuracy: 0.8465 - val_loss: 1.9016 - val_accuracy: 0.8211\n",
      "Epoch 2/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 1.4142 - accuracy: 0.8490 - val_loss: 1.9519 - val_accuracy: 0.8208\n",
      "Epoch 3/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 1.3433 - accuracy: 0.8496 - val_loss: 1.9942 - val_accuracy: 0.8221\n",
      "Epoch 4/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 1.2717 - accuracy: 0.8508 - val_loss: 2.0402 - val_accuracy: 0.8233\n",
      "Epoch 5/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 1.1993 - accuracy: 0.8532 - val_loss: 2.0586 - val_accuracy: 0.8247\n",
      "Epoch 6/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 1.1218 - accuracy: 0.8569 - val_loss: 2.0850 - val_accuracy: 0.8256\n",
      "Epoch 7/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 1.0456 - accuracy: 0.8607 - val_loss: 2.1069 - val_accuracy: 0.8271\n",
      "Epoch 8/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.9803 - accuracy: 0.8645 - val_loss: 2.1247 - val_accuracy: 0.8279\n",
      "Epoch 9/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.9197 - accuracy: 0.8689 - val_loss: 2.1384 - val_accuracy: 0.8290\n",
      "Epoch 10/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.8656 - accuracy: 0.8724 - val_loss: 2.1576 - val_accuracy: 0.8294\n",
      "Epoch 11/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.8214 - accuracy: 0.8768 - val_loss: 2.1705 - val_accuracy: 0.8305\n",
      "Epoch 12/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.7755 - accuracy: 0.8809 - val_loss: 2.1938 - val_accuracy: 0.8299\n",
      "Epoch 13/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.7400 - accuracy: 0.8839 - val_loss: 2.2084 - val_accuracy: 0.8308\n",
      "Epoch 14/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.7038 - accuracy: 0.8882 - val_loss: 2.2250 - val_accuracy: 0.8321\n",
      "Epoch 15/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.6755 - accuracy: 0.8913 - val_loss: 2.2398 - val_accuracy: 0.8319\n",
      "Epoch 16/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.6486 - accuracy: 0.8941 - val_loss: 2.2559 - val_accuracy: 0.8313\n",
      "Epoch 17/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.6244 - accuracy: 0.8967 - val_loss: 2.2762 - val_accuracy: 0.8315\n",
      "Epoch 18/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.6081 - accuracy: 0.8989 - val_loss: 2.2873 - val_accuracy: 0.8333\n",
      "Epoch 19/20\n",
      "371/371 [==============================] - 20s 55ms/step - loss: 0.5834 - accuracy: 0.9028 - val_loss: 2.3046 - val_accuracy: 0.8332\n",
      "Epoch 20/20\n",
      "371/371 [==============================] - 21s 55ms/step - loss: 0.5670 - accuracy: 0.9046 - val_loss: 2.3221 - val_accuracy: 0.8330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2463bb67a00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bidirectional_embed_model(input_shape, output_sequence_length, french_vocab_size, tamil_vocab_size):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 5e-3\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(french_vocab_size, 32, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(tamil_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate, clipnorm=1.0),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Prepping the input layer \n",
    "tmp_x = pad(preproc_french_sentences, max_tamil_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-1]))\n",
    "\n",
    "# Build the model\n",
    "embed_rnn_model = bidirectional_embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_tamil_sequence_length,\n",
    "    french_vocab_size,\n",
    "    tamil_vocab_size)\n",
    "\n",
    "print(embed_rnn_model.summary())\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_tamil_sentences, batch_size=32, epochs=20, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9345e-0af4-4328-babb-cf8bfa336138",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc9e508-e791-4297-ade5-d5dbbd41ca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 465ms/step\n",
      "[1362, 0, 0, 0, 0, 0, 0]\n",
      "Predicted Tamil Sentences மதரஸா\n",
      "Actual Tamil Sentences ஆப்பிள்\n"
     ]
    }
   ],
   "source": [
    "french_sentence = [\"pomme\"]\n",
    "french_sentence = french_sentence[0].lower()\n",
    "\n",
    "actual_translation = [\"ஆப்பிள்\"]\n",
    "\n",
    "french_sentence = french_tokenizer.texts_to_sequences([french_sentence])\n",
    "french_sentence = pad_sequences(french_sentence,max_tamil_sequence_length,padding=\"post\")\n",
    "\n",
    "\n",
    "tamil_sentence = embed_rnn_model.predict(french_sentence)[0]\n",
    "\n",
    "tamil_sentence = [np.argmax(word) for word in tamil_sentence]\n",
    "print(tamil_sentence)\n",
    "\n",
    "tamil_sentence = tamil_tokenizer.sequences_to_texts([tamil_sentence])[0]\n",
    "\n",
    "print(\"Predicted Tamil Sentences\", tamil_sentence)\n",
    "print(\"Actual Tamil Sentences\", actual_translation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4921b3-c0c3-4bf0-8414-2143c9ca820e",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e93dc4-e859-4a03-ab26-f01ed607433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: french_to_tamil_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: french_to_tamil_model\\assets\n"
     ]
    }
   ],
   "source": [
    "embed_rnn_model.save('french_to_tamil_model')\n",
    "# Serialize English Tokenizer to JSON\n",
    "with open('french_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(french_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Serialize French Tokenizer to JSON\n",
    "with open('tamil_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(tamil_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Save max lengths\n",
    "max_tamil_sequence_length_json = max_tamil_sequence_length\n",
    "with open('sequence_length.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(max_tamil_sequence_length_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46bfee-6fef-4571-aab5-e4d714851515",
   "metadata": {},
   "source": [
    "## Run the GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985f1b0a-9d8f-4f90-b491-e8fe44ba68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gui.py file using the above created model as the means of translation \n",
    "%run gui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2206f04-200e-412c-bd32-d17f41e3b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gui_2.py file uses the googletrans library for the translation purposes\n",
    "%run gui_2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
